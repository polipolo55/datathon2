{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52c800b6",
   "metadata": {},
   "source": [
    "# CatBoost CV Upgrade\n",
    "Enhanced notebook with richer feature engineering, robust cross-validation, and calibrated CatBoost inference for the Datathon production challenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43f3f199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatBoost GPU detected on device(s): 0\n",
      "Train shape: (95339, 33) | Test shape: (2250, 28)\n"
     ]
    }
   ],
   "source": [
    "# Imports and configuration\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "from catboost.utils import get_gpu_device_count\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "SEED = 42\n",
    "GPU_DEVICES = \"0\"  # set to \"0,1\" to leverage multiple GPUs\n",
    "GPU_AVAILABLE = get_gpu_device_count() > 0\n",
    "CATBOOST_TASK_TYPE = \"GPU\" if GPU_AVAILABLE else \"CPU\"\n",
    "if GPU_AVAILABLE:\n",
    "    print(f\"CatBoost GPU detected on device(s): {GPU_DEVICES}\")\n",
    "else:\n",
    "    print(\"CatBoost GPU not detected; falling back to CPU training.\")\n",
    "\n",
    "DATA_DIR = Path(\"data\")\n",
    "SUBMISSION_DIR = Path(\"submissions\")\n",
    "SUBMISSION_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TRAIN_PATH = DATA_DIR / \"train.csv\"\n",
    "TEST_PATH = DATA_DIR / \"test.csv\"\n",
    "SAMPLE_SUB_PATH = DATA_DIR / \"sample_submission.csv\"\n",
    "\n",
    "for required_path in [TRAIN_PATH, TEST_PATH]:\n",
    "    if not required_path.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"Missing required dataset: {required_path.resolve()} â€” \"\n",
    "            \"please place the competition CSVs inside data/.\"\n",
    ")\n",
    "\n",
    "def load_dataset(path: Path) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path, sep=\";\")\n",
    "    df = df.loc[:, ~df.columns.str.contains(\"^Unnamed\")]\n",
    "    return df\n",
    "\n",
    "train_df = load_dataset(TRAIN_PATH)\n",
    "test_df = load_dataset(TEST_PATH)\n",
    "sample_submission = load_dataset(SAMPLE_SUB_PATH) if SAMPLE_SUB_PATH.exists() else None\n",
    "\n",
    "print(f\"Train shape: {train_df.shape} | Test shape: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66316bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper utilities for feature engineering\n",
    "def parse_embedding_vector(raw_value):\n",
    "    if pd.isna(raw_value) or raw_value == \"\":\n",
    "        return np.zeros(1, dtype=np.float32)\n",
    "    text = str(raw_value).replace(\"[\", \"\").replace(\"]\", \"\")\n",
    "    values = [part.strip() for part in text.split(\",\") if part.strip() != \"\"]\n",
    "    if not values:\n",
    "        return np.zeros(1, dtype=np.float32)\n",
    "    try:\n",
    "        return np.array([float(v) for v in values], dtype=np.float32)\n",
    "    except ValueError:\n",
    "        return np.zeros(len(values) or 1, dtype=np.float32)\n",
    "\n",
    "def build_embedding_matrix(series: pd.Series) -> np.ndarray:\n",
    "    parsed = [parse_embedding_vector(value) for value in series]\n",
    "    max_dim = max((vec.size for vec in parsed), default=1)\n",
    "    matrix = np.zeros((len(parsed), max_dim), dtype=np.float32)\n",
    "    for row_idx, vec in enumerate(parsed):\n",
    "        matrix[row_idx, : vec.size] = vec\n",
    "    return matrix\n",
    "\n",
    "def add_embedding_features(df: pd.DataFrame, column: str = \"image_embedding\", n_components: int = 48) -> pd.DataFrame:\n",
    "    if column not in df.columns:\n",
    "        return df\n",
    "    df = df.copy()\n",
    "    emb_matrix = build_embedding_matrix(df[column])\n",
    "    emb_numeric = np.nan_to_num(emb_matrix)\n",
    "    df[\"img_emb_mean\"] = emb_numeric.mean(axis=1)\n",
    "    df[\"img_emb_std\"] = emb_numeric.std(axis=1)\n",
    "    df[\"img_emb_abs_mean\"] = np.abs(emb_numeric).mean(axis=1)\n",
    "    df[\"img_emb_max\"] = emb_numeric.max(axis=1)\n",
    "    df[\"img_emb_min\"] = emb_numeric.min(axis=1)\n",
    "    df[\"img_emb_median\"] = np.nanmedian(emb_numeric, axis=1)\n",
    "    df[\"img_emb_q25\"] = np.nanquantile(emb_numeric, 0.25, axis=1)\n",
    "    df[\"img_emb_q75\"] = np.nanquantile(emb_numeric, 0.75, axis=1)\n",
    "    df[\"img_emb_energy\"] = (emb_numeric ** 2).sum(axis=1)\n",
    "    df[\"img_emb_norm\"] = np.linalg.norm(emb_numeric, axis=1)\n",
    "\n",
    "    usable_components = min(n_components, emb_matrix.shape[1], max(1, emb_matrix.shape[0] - 1))\n",
    "    emb_pca = None\n",
    "    if emb_matrix.shape[0] > 1 and usable_components >= 1:\n",
    "        pca = PCA(n_components=usable_components, random_state=SEED)\n",
    "        emb_pca = pca.fit_transform(emb_numeric)\n",
    "        for comp_idx in range(usable_components):\n",
    "            df[f\"img_pca_{comp_idx}\"] = emb_pca[:, comp_idx]\n",
    "\n",
    "    cluster_source = emb_pca if emb_pca is not None and emb_pca.shape[1] >= 2 else emb_numeric\n",
    "    if cluster_source.shape[0] >= 2:\n",
    "        cluster_count = min(8, max(2, cluster_source.shape[0]))\n",
    "        try:\n",
    "            kmeans = KMeans(n_clusters=cluster_count, random_state=SEED, n_init=10)\n",
    "            cluster_labels = kmeans.fit_predict(cluster_source)\n",
    "            df[\"img_emb_cluster\"] = cluster_labels.astype(str)\n",
    "            df[\"img_emb_cluster_dist\"] = kmeans.transform(cluster_source).min(axis=1)\n",
    "        except ValueError:\n",
    "            df[\"img_emb_cluster\"] = \"0\"\n",
    "            df[\"img_emb_cluster_dist\"] = 0.0\n",
    "\n",
    "    df = df.drop(columns=[column])\n",
    "    return df\n",
    "\n",
    "def safe_ratio(numerator: pd.Series, denominator: pd.Series) -> pd.Series:\n",
    "    denominator = denominator.replace(0, np.nan)\n",
    "    ratio = numerator / denominator\n",
    "    return ratio.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "def engineer_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "\n",
    "    for numeric_col in [\"num_stores\", \"num_sizes\", \"weekly_demand\"]:\n",
    "        if numeric_col in df.columns:\n",
    "            df[numeric_col] = pd.to_numeric(df[numeric_col], errors=\"coerce\")\n",
    "\n",
    "    for col in [\"phase_in\", \"phase_out\"]:\n",
    "        if col in df.columns:\n",
    "            df[f\"{col}_dt\"] = pd.to_datetime(df[col], format=\"%d/%m/%Y\", errors=\"coerce\")\n",
    "\n",
    "    if \"phase_in_dt\" in df.columns:\n",
    "        df[\"phase_in_year\"] = df[\"phase_in_dt\"].dt.year\n",
    "        df[\"phase_in_month\"] = df[\"phase_in_dt\"].dt.month\n",
    "        df[\"phase_in_day\"] = df[\"phase_in_dt\"].dt.day\n",
    "        df[\"phase_in_dayofyear\"] = df[\"phase_in_dt\"].dt.dayofyear\n",
    "        df[\"phase_in_week\"] = df[\"phase_in_dt\"].dt.isocalendar().week.astype(float)\n",
    "        df[\"phase_in_weekday\"] = df[\"phase_in_dt\"].dt.weekday\n",
    "        df[\"phase_in_weekend\"] = (df[\"phase_in_dt\"].dt.weekday >= 5).astype(int)\n",
    "        df[\"phase_in_quarter\"] = ((df[\"phase_in_month\"] - 1) // 3 + 1).astype(float)\n",
    "        base_launch = df[\"phase_in_dt\"].min()\n",
    "        if pd.notna(base_launch):\n",
    "            df[\"phase_in_age_days\"] = (df[\"phase_in_dt\"] - base_launch).dt.days\n",
    "\n",
    "    if \"phase_out_dt\" in df.columns:\n",
    "        df[\"phase_out_year\"] = df[\"phase_out_dt\"].dt.year\n",
    "        df[\"phase_out_month\"] = df[\"phase_out_dt\"].dt.month\n",
    "        df[\"phase_out_week\"] = df[\"phase_out_dt\"].dt.isocalendar().week.astype(float)\n",
    "        min_phase_out = df[\"phase_out_dt\"].min()\n",
    "        if pd.notna(min_phase_out):\n",
    "            df[\"phase_out_age_days\"] = (df[\"phase_out_dt\"] - min_phase_out).dt.days\n",
    "\n",
    "    if {\"phase_in_dt\", \"phase_out_dt\"}.issubset(df.columns):\n",
    "        df[\"lifecycle_days\"] = (df[\"phase_out_dt\"] - df[\"phase_in_dt\"]).dt.days\n",
    "        df[\"lifecycle_days\"] = df[\"lifecycle_days\"].clip(lower=0)\n",
    "        df[\"lifecycle_weeks\"] = df[\"lifecycle_days\"] / 7.0\n",
    "    else:\n",
    "        df[\"lifecycle_days\"] = np.nan\n",
    "        df[\"lifecycle_weeks\"] = np.nan\n",
    "\n",
    "    df[\"lifecycle_missing\"] = df[\"lifecycle_days\"].isna().astype(int)\n",
    "    df[\"lifecycle_days\"] = df[\"lifecycle_days\"].fillna(df[\"lifecycle_days\"].median())\n",
    "    df[\"lifecycle_weeks\"] = df[\"lifecycle_weeks\"].fillna(df[\"lifecycle_weeks\"].median())\n",
    "\n",
    "    if \"phase_in_month\" in df.columns:\n",
    "        season_map = {\n",
    "            \"winter\": [12, 1, 2],\n",
    "            \"spring\": [3, 4, 5],\n",
    "            \"summer\": [6, 7, 8],\n",
    "            \"fall\": [9, 10, 11],\n",
    "        }\n",
    "        for season_name, months in season_map.items():\n",
    "            df[f\"launch_{season_name}\"] = df[\"phase_in_month\"].isin(months).astype(int)\n",
    "        df[\"phase_in_year_progress\"] = df[\"phase_in_dayofyear\"] / 365.0\n",
    "\n",
    "    for cyc_col, period in [(\"phase_in_month\", 12), (\"phase_in_dayofyear\", 365), (\"phase_in_week\", 52)]:\n",
    "        if cyc_col in df.columns:\n",
    "            angle = 2 * np.pi * df[cyc_col].fillna(0) / period\n",
    "            df[f\"{cyc_col}_sin\"] = np.sin(angle)\n",
    "            df[f\"{cyc_col}_cos\"] = np.cos(angle)\n",
    "\n",
    "    if \"color_rgb\" in df.columns:\n",
    "        def parse_rgb(value):\n",
    "            if pd.isna(value) or value == \"\":\n",
    "                return [128, 128, 128]\n",
    "            try:\n",
    "                parts = [int(float(x)) for x in str(value).split(\",\")]\n",
    "                return parts if len(parts) == 3 else [128, 128, 128]\n",
    "            except ValueError:\n",
    "                return [128, 128, 128]\n",
    "\n",
    "        rgb_values = np.array(df[\"color_rgb\"].apply(parse_rgb).tolist())\n",
    "        df[\"color_r\"] = rgb_values[:, 0]\n",
    "        df[\"color_g\"] = rgb_values[:, 1]\n",
    "        df[\"color_b\"] = rgb_values[:, 2]\n",
    "        df[\"color_mean\"] = rgb_values.mean(axis=1)\n",
    "        df[\"color_std\"] = rgb_values.std(axis=1)\n",
    "        df[\"color_range\"] = np.ptp(rgb_values, axis=1)\n",
    "        df[\"is_dark_color\"] = (df[\"color_mean\"] < 90).astype(int)\n",
    "\n",
    "    ratio_specs = [\n",
    "        (\"weekly_demand\", \"num_stores\", \"demand_per_store\"),\n",
    "        (\"weekly_demand\", \"num_sizes\", \"demand_per_size\"),\n",
    "        (\"num_stores\", \"num_sizes\", \"stores_per_size\"),\n",
    "        (\"weekly_demand\", \"lifecycle_weeks\", \"demand_per_week\"),\n",
    "        (\"num_stores\", \"lifecycle_weeks\", \"stores_per_week\"),\n",
    "        (\"num_sizes\", \"lifecycle_weeks\", \"sizes_per_week\"),\n",
    "        (\"lifecycle_days\", \"num_stores\", \"lifecycle_per_store\"),\n",
    "        (\"lifecycle_days\", \"num_sizes\", \"lifecycle_per_size\"),\n",
    "        (\"num_stores\", \"weekly_demand\", \"stores_to_demand\"),\n",
    "        (\"num_sizes\", \"weekly_demand\", \"sizes_to_demand\"),\n",
    "    ]\n",
    "    for numerator_col, denominator_col, feature_name in ratio_specs:\n",
    "        if numerator_col in df.columns and denominator_col in df.columns:\n",
    "            df[feature_name] = safe_ratio(df[numerator_col], df[denominator_col])\n",
    "\n",
    "    if {\"num_stores\", \"num_sizes\"}.issubset(df.columns):\n",
    "        df[\"stores_times_sizes\"] = df[\"num_stores\"] * df[\"num_sizes\"]\n",
    "        df[\"stores_minus_sizes\"] = df[\"num_stores\"] - df[\"num_sizes\"]\n",
    "        df[\"stores_plus_sizes\"] = df[\"num_stores\"] + df[\"num_sizes\"]\n",
    "        if \"weekly_demand\" in df.columns:\n",
    "            df[\"demand_per_store_size\"] = safe_ratio(df[\"weekly_demand\"], df[\"stores_times_sizes\"])\n",
    "\n",
    "    if {\"weekly_demand\", \"lifecycle_weeks\"}.issubset(df.columns):\n",
    "        df[\"demand_times_lifecycle\"] = df[\"weekly_demand\"] * df[\"lifecycle_weeks\"]\n",
    "\n",
    "    log_candidates = [\n",
    "        \"weekly_demand\",\n",
    "        \"num_stores\",\n",
    "        \"num_sizes\",\n",
    "        \"lifecycle_days\",\n",
    "        \"demand_per_store\",\n",
    "        \"demand_per_size\",\n",
    "        \"demand_per_week\",\n",
    "        \"demand_per_store_size\",\n",
    "    ]\n",
    "    for col in log_candidates:\n",
    "        if col in df.columns:\n",
    "            df[f\"log_{col}\"] = np.log1p(df[col].clip(lower=0))\n",
    "\n",
    "    for col in [\"product_name\", \"product_type\", \"designer\", \"theme\", \"description\"]:\n",
    "        if col in df.columns:\n",
    "            text_series = df[col].fillna(\"\").astype(str)\n",
    "            df[f\"{col}_len\"] = text_series.str.len()\n",
    "            df[f\"{col}_words\"] = text_series.str.split().str.len()\n",
    "\n",
    "    categorical_cols = df.select_dtypes(include=[\"object\", \"category\"]).columns\n",
    "    for col in categorical_cols:\n",
    "        freq = df[col].value_counts(dropna=False, normalize=True)\n",
    "        df[f\"{col}_freq\"] = df[col].map(freq)\n",
    "\n",
    "    protected_missing_cols = {\"Production\", \"is_train\"}\n",
    "    missing_cols = [col for col in df.columns if col not in protected_missing_cols and df[col].isna().any()]\n",
    "    for col in missing_cols:\n",
    "        df[f\"{col}_missing_flag\"] = df[col].isna().astype(int)\n",
    "    df[\"row_missing_count\"] = df.isna().sum(axis=1)\n",
    "    df[\"row_missing_ratio\"] = df[\"row_missing_count\"] / max(1, df.shape[1])\n",
    "\n",
    "    df = df.drop(columns=[\"phase_in\", \"phase_out\", \"color_rgb\", \"phase_in_dt\", \"phase_out_dt\"], errors=\"ignore\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e0dc81b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sterr\\AppData\\Local\\Temp\\ipykernel_45104\\1927523700.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_missing_flag\"] = df[col].isna().astype(int)\n",
      "C:\\Users\\sterr\\AppData\\Local\\Temp\\ipykernel_45104\\1927523700.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_missing_flag\"] = df[col].isna().astype(int)\n",
      "C:\\Users\\sterr\\AppData\\Local\\Temp\\ipykernel_45104\\1927523700.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_missing_flag\"] = df[col].isna().astype(int)\n",
      "C:\\Users\\sterr\\AppData\\Local\\Temp\\ipykernel_45104\\1927523700.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_missing_flag\"] = df[col].isna().astype(int)\n",
      "C:\\Users\\sterr\\AppData\\Local\\Temp\\ipykernel_45104\\1927523700.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_missing_flag\"] = df[col].isna().astype(int)\n",
      "C:\\Users\\sterr\\AppData\\Local\\Temp\\ipykernel_45104\\1927523700.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_missing_flag\"] = df[col].isna().astype(int)\n",
      "C:\\Users\\sterr\\AppData\\Local\\Temp\\ipykernel_45104\\1927523700.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_missing_flag\"] = df[col].isna().astype(int)\n",
      "C:\\Users\\sterr\\AppData\\Local\\Temp\\ipykernel_45104\\1927523700.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_missing_flag\"] = df[col].isna().astype(int)\n",
      "C:\\Users\\sterr\\AppData\\Local\\Temp\\ipykernel_45104\\1927523700.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_missing_flag\"] = df[col].isna().astype(int)\n",
      "C:\\Users\\sterr\\AppData\\Local\\Temp\\ipykernel_45104\\1927523700.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_missing_flag\"] = df[col].isna().astype(int)\n",
      "C:\\Users\\sterr\\AppData\\Local\\Temp\\ipykernel_45104\\1927523700.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_missing_flag\"] = df[col].isna().astype(int)\n",
      "C:\\Users\\sterr\\AppData\\Local\\Temp\\ipykernel_45104\\1927523700.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_missing_flag\"] = df[col].isna().astype(int)\n",
      "C:\\Users\\sterr\\AppData\\Local\\Temp\\ipykernel_45104\\1927523700.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_missing_flag\"] = df[col].isna().astype(int)\n",
      "C:\\Users\\sterr\\AppData\\Local\\Temp\\ipykernel_45104\\1927523700.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_missing_flag\"] = df[col].isna().astype(int)\n",
      "C:\\Users\\sterr\\AppData\\Local\\Temp\\ipykernel_45104\\1927523700.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_missing_flag\"] = df[col].isna().astype(int)\n",
      "C:\\Users\\sterr\\AppData\\Local\\Temp\\ipykernel_45104\\1927523700.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"row_missing_count\"] = df.isna().sum(axis=1)\n",
      "C:\\Users\\sterr\\AppData\\Local\\Temp\\ipykernel_45104\\1927523700.py:203: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"row_missing_ratio\"] = df[\"row_missing_count\"] / max(1, df.shape[1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final training matrix: (95339, 192) | Test matrix: (2250, 192)\n",
      "Categorical features tracked: 16 | Numeric features: 176\n"
     ]
    }
   ],
   "source": [
    "# Feature engineering and dataset assembly\n",
    "train_df = train_df.copy()\n",
    "test_df = test_df.copy()\n",
    "\n",
    "train_df[\"is_train\"] = 1\n",
    "test_df[\"is_train\"] = 0\n",
    "test_df[\"Production\"] = np.nan\n",
    "\n",
    "full_df = pd.concat([train_df, test_df], ignore_index=True)\n",
    "full_df = add_embedding_features(full_df, column=\"image_embedding\", n_components=48)\n",
    "full_df = engineer_features(full_df)\n",
    "\n",
    "train_processed = full_df[full_df[\"is_train\"] == 1].drop(columns=[\"is_train\"])\n",
    "test_processed = full_df[full_df[\"is_train\"] == 0].drop(columns=[\"is_train\"])\n",
    "\n",
    "test_ids = test_processed[\"ID\"].copy() if \"ID\" in test_processed.columns else pd.Series(np.arange(len(test_processed)))\n",
    "\n",
    "if \"ID\" in train_processed.columns:\n",
    "    train_processed = train_processed.drop(columns=[\"ID\"])\n",
    "    test_processed = test_processed.drop(columns=[\"ID\"])\n",
    "\n",
    "y = train_processed[\"Production\"].astype(float)\n",
    "X = train_processed.drop(columns=[\"Production\"]).reset_index(drop=True)\n",
    "test_features = test_processed.drop(columns=[\"Production\"], errors=\"ignore\").reset_index(drop=True)\n",
    "\n",
    "numeric_cols = X.select_dtypes(include=[np.number]).columns\n",
    "numeric_medians = X[numeric_cols].median().fillna(0)\n",
    "\n",
    "X[numeric_cols] = X[numeric_cols].fillna(numeric_medians)\n",
    "for col in numeric_cols:\n",
    "    fill_value = numeric_medians[col] if col in numeric_medians else 0\n",
    "    if col in test_features.columns:\n",
    "        test_features[col] = test_features[col].fillna(fill_value)\n",
    "    else:\n",
    "        test_features[col] = fill_value\n",
    "\n",
    "missing_test_cols = [col for col in X.columns if col not in test_features.columns]\n",
    "for col in missing_test_cols:\n",
    "    test_features[col] = 0\n",
    "\n",
    "extra_test_cols = [col for col in test_features.columns if col not in X.columns]\n",
    "if extra_test_cols:\n",
    "    test_features = test_features.drop(columns=extra_test_cols)\n",
    "\n",
    "test_features = test_features[X.columns]\n",
    "\n",
    "X[numeric_cols] = X[numeric_cols].astype(np.float32)\n",
    "test_features[numeric_cols] = test_features[numeric_cols].astype(np.float32)\n",
    "\n",
    "categorical_cols = X.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "for col in categorical_cols:\n",
    "    X[col] = X[col].astype(str).fillna(\"missing\")\n",
    "    if col in test_features.columns:\n",
    "        test_features[col] = test_features[col].astype(str).fillna(\"missing\")\n",
    "\n",
    "cat_feature_indices = [X.columns.get_loc(col) for col in categorical_cols]\n",
    "cat_features_for_pool = cat_feature_indices if categorical_cols else None\n",
    "\n",
    "print(f\"Final training matrix: {X.shape} | Test matrix: {test_features.shape}\")\n",
    "print(f\"Categorical features tracked: {len(categorical_cols)} | Numeric features: {len(X.columns) - len(categorical_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "283f5877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 34215.3790280\ttest: 33569.9394330\tbest: 33569.9394330 (0)\ttotal: 104ms\tremaining: 3m 27s\n",
      "250:\tlearn: 6999.2317910\ttest: 7350.7976290\tbest: 7350.7976290 (250)\ttotal: 15s\tremaining: 1m 44s\n",
      "250:\tlearn: 6999.2317910\ttest: 7350.7976290\tbest: 7350.7976290 (250)\ttotal: 15s\tremaining: 1m 44s\n",
      "500:\tlearn: 5354.8896995\ttest: 5757.2497735\tbest: 5757.2497735 (500)\ttotal: 30.1s\tremaining: 1m 30s\n",
      "500:\tlearn: 5354.8896995\ttest: 5757.2497735\tbest: 5757.2497735 (500)\ttotal: 30.1s\tremaining: 1m 30s\n",
      "750:\tlearn: 4382.8612567\ttest: 4790.7473537\tbest: 4790.7473537 (750)\ttotal: 45.3s\tremaining: 1m 15s\n",
      "750:\tlearn: 4382.8612567\ttest: 4790.7473537\tbest: 4790.7473537 (750)\ttotal: 45.3s\tremaining: 1m 15s\n",
      "1000:\tlearn: 3711.7183077\ttest: 4128.2167038\tbest: 4128.2167038 (1000)\ttotal: 1m\tremaining: 1m\n",
      "1000:\tlearn: 3711.7183077\ttest: 4128.2167038\tbest: 4128.2167038 (1000)\ttotal: 1m\tremaining: 1m\n",
      "1250:\tlearn: 3226.4173046\ttest: 3648.5937416\tbest: 3648.5937416 (1250)\ttotal: 1m 15s\tremaining: 44.9s\n",
      "1250:\tlearn: 3226.4173046\ttest: 3648.5937416\tbest: 3648.5937416 (1250)\ttotal: 1m 15s\tremaining: 44.9s\n",
      "1500:\tlearn: 2832.9419926\ttest: 3259.3232124\tbest: 3259.3232124 (1500)\ttotal: 1m 30s\tremaining: 30s\n",
      "1500:\tlearn: 2832.9419926\ttest: 3259.3232124\tbest: 3259.3232124 (1500)\ttotal: 1m 30s\tremaining: 30s\n",
      "1750:\tlearn: 2527.7778297\ttest: 2958.9398472\tbest: 2958.9398472 (1750)\ttotal: 1m 45s\tremaining: 15s\n",
      "1750:\tlearn: 2527.7778297\ttest: 2958.9398472\tbest: 2958.9398472 (1750)\ttotal: 1m 45s\tremaining: 15s\n",
      "1999:\tlearn: 2275.0984960\ttest: 2709.3199077\tbest: 2709.3199077 (1999)\ttotal: 2m\tremaining: 0us\n",
      "bestTest = 2709.319908\n",
      "bestIteration = 1999\n",
      "Fold 1 RMSE: 2709.3195\n",
      "1999:\tlearn: 2275.0984960\ttest: 2709.3199077\tbest: 2709.3199077 (1999)\ttotal: 2m\tremaining: 0us\n",
      "bestTest = 2709.319908\n",
      "bestIteration = 1999\n",
      "Fold 1 RMSE: 2709.3195\n",
      "0:\tlearn: 33995.5101595\ttest: 34471.6017638\tbest: 34471.6017638 (0)\ttotal: 48.9ms\tremaining: 1m 37s\n",
      "0:\tlearn: 33995.5101595\ttest: 34471.6017638\tbest: 34471.6017638 (0)\ttotal: 48.9ms\tremaining: 1m 37s\n",
      "250:\tlearn: 7023.1551249\ttest: 7250.1399355\tbest: 7250.1399355 (250)\ttotal: 15.2s\tremaining: 1m 46s\n",
      "250:\tlearn: 7023.1551249\ttest: 7250.1399355\tbest: 7250.1399355 (250)\ttotal: 15.2s\tremaining: 1m 46s\n",
      "500:\tlearn: 5347.8069126\ttest: 5598.7941907\tbest: 5598.7941907 (500)\ttotal: 30.5s\tremaining: 1m 31s\n",
      "500:\tlearn: 5347.8069126\ttest: 5598.7941907\tbest: 5598.7941907 (500)\ttotal: 30.5s\tremaining: 1m 31s\n",
      "750:\tlearn: 4386.5211240\ttest: 4653.4862422\tbest: 4653.4862422 (750)\ttotal: 45.7s\tremaining: 1m 15s\n",
      "750:\tlearn: 4386.5211240\ttest: 4653.4862422\tbest: 4653.4862422 (750)\ttotal: 45.7s\tremaining: 1m 15s\n",
      "1000:\tlearn: 3753.0513824\ttest: 4038.1560684\tbest: 4038.1560684 (1000)\ttotal: 1m\tremaining: 1m\n",
      "1000:\tlearn: 3753.0513824\ttest: 4038.1560684\tbest: 4038.1560684 (1000)\ttotal: 1m\tremaining: 1m\n",
      "1250:\tlearn: 3279.7791180\ttest: 3575.6588483\tbest: 3575.6588483 (1250)\ttotal: 1m 15s\tremaining: 45.1s\n",
      "1250:\tlearn: 3279.7791180\ttest: 3575.6588483\tbest: 3575.6588483 (1250)\ttotal: 1m 15s\tremaining: 45.1s\n",
      "1500:\tlearn: 2897.6397893\ttest: 3203.6508982\tbest: 3203.6508982 (1500)\ttotal: 1m 30s\tremaining: 30s\n",
      "1500:\tlearn: 2897.6397893\ttest: 3203.6508982\tbest: 3203.6508982 (1500)\ttotal: 1m 30s\tremaining: 30s\n",
      "1750:\tlearn: 2578.0502171\ttest: 2887.7960073\tbest: 2887.7960073 (1750)\ttotal: 1m 45s\tremaining: 15s\n",
      "1750:\tlearn: 2578.0502171\ttest: 2887.7960073\tbest: 2887.7960073 (1750)\ttotal: 1m 45s\tremaining: 15s\n",
      "1999:\tlearn: 2321.3633269\ttest: 2635.2613090\tbest: 2635.2613090 (1999)\ttotal: 2m\tremaining: 0us\n",
      "bestTest = 2635.261309\n",
      "bestIteration = 1999\n",
      "Fold 2 RMSE: 2635.2616\n",
      "1999:\tlearn: 2321.3633269\ttest: 2635.2613090\tbest: 2635.2613090 (1999)\ttotal: 2m\tremaining: 0us\n",
      "bestTest = 2635.261309\n",
      "bestIteration = 1999\n",
      "Fold 2 RMSE: 2635.2616\n",
      "0:\tlearn: 34035.8557658\ttest: 34412.6429291\tbest: 34412.6429291 (0)\ttotal: 54.6ms\tremaining: 1m 49s\n",
      "0:\tlearn: 34035.8557658\ttest: 34412.6429291\tbest: 34412.6429291 (0)\ttotal: 54.6ms\tremaining: 1m 49s\n",
      "250:\tlearn: 7027.6948804\ttest: 7069.8079558\tbest: 7069.8079558 (250)\ttotal: 16.3s\tremaining: 1m 53s\n",
      "250:\tlearn: 7027.6948804\ttest: 7069.8079558\tbest: 7069.8079558 (250)\ttotal: 16.3s\tremaining: 1m 53s\n",
      "500:\tlearn: 5357.3284749\ttest: 5481.3784641\tbest: 5481.3784641 (500)\ttotal: 32.8s\tremaining: 1m 38s\n",
      "500:\tlearn: 5357.3284749\ttest: 5481.3784641\tbest: 5481.3784641 (500)\ttotal: 32.8s\tremaining: 1m 38s\n",
      "750:\tlearn: 4369.6038383\ttest: 4541.4621300\tbest: 4541.4621300 (750)\ttotal: 48.9s\tremaining: 1m 21s\n",
      "750:\tlearn: 4369.6038383\ttest: 4541.4621300\tbest: 4541.4621300 (750)\ttotal: 48.9s\tremaining: 1m 21s\n",
      "1000:\tlearn: 3700.7929294\ttest: 3905.5081118\tbest: 3905.5081118 (1000)\ttotal: 1m 4s\tremaining: 1m 4s\n",
      "1000:\tlearn: 3700.7929294\ttest: 3905.5081118\tbest: 3905.5081118 (1000)\ttotal: 1m 4s\tremaining: 1m 4s\n",
      "1250:\tlearn: 3214.3773892\ttest: 3438.9828812\tbest: 3438.9828812 (1250)\ttotal: 1m 20s\tremaining: 48s\n",
      "1250:\tlearn: 3214.3773892\ttest: 3438.9828812\tbest: 3438.9828812 (1250)\ttotal: 1m 20s\tremaining: 48s\n",
      "1500:\tlearn: 2829.9531427\ttest: 3072.4687426\tbest: 3072.4687426 (1500)\ttotal: 1m 34s\tremaining: 31.6s\n",
      "1500:\tlearn: 2829.9531427\ttest: 3072.4687426\tbest: 3072.4687426 (1500)\ttotal: 1m 34s\tremaining: 31.6s\n",
      "1750:\tlearn: 2507.8974759\ttest: 2762.8084227\tbest: 2762.8084227 (1750)\ttotal: 1m 49s\tremaining: 15.6s\n",
      "1750:\tlearn: 2507.8974759\ttest: 2762.8084227\tbest: 2762.8084227 (1750)\ttotal: 1m 49s\tremaining: 15.6s\n",
      "1999:\tlearn: 2247.3934874\ttest: 2511.8763550\tbest: 2511.8763550 (1999)\ttotal: 2m 4s\tremaining: 0us\n",
      "bestTest = 2511.876355\n",
      "bestIteration = 1999\n",
      "Fold 3 RMSE: 2511.8766\n",
      "1999:\tlearn: 2247.3934874\ttest: 2511.8763550\tbest: 2511.8763550 (1999)\ttotal: 2m 4s\tremaining: 0us\n",
      "bestTest = 2511.876355\n",
      "bestIteration = 1999\n",
      "Fold 3 RMSE: 2511.8766\n",
      "0:\tlearn: 34121.6243762\ttest: 33984.0510856\tbest: 33984.0510856 (0)\ttotal: 48.1ms\tremaining: 1m 36s\n",
      "0:\tlearn: 34121.6243762\ttest: 33984.0510856\tbest: 33984.0510856 (0)\ttotal: 48.1ms\tremaining: 1m 36s\n",
      "250:\tlearn: 7042.7217553\ttest: 7167.4840535\tbest: 7167.4840535 (250)\ttotal: 15.1s\tremaining: 1m 45s\n",
      "250:\tlearn: 7042.7217553\ttest: 7167.4840535\tbest: 7167.4840535 (250)\ttotal: 15.1s\tremaining: 1m 45s\n",
      "500:\tlearn: 5355.2205602\ttest: 5541.9917496\tbest: 5541.9917496 (500)\ttotal: 30.3s\tremaining: 1m 30s\n",
      "500:\tlearn: 5355.2205602\ttest: 5541.9917496\tbest: 5541.9917496 (500)\ttotal: 30.3s\tremaining: 1m 30s\n",
      "750:\tlearn: 4383.8472680\ttest: 4614.2332966\tbest: 4614.2332966 (750)\ttotal: 45.5s\tremaining: 1m 15s\n",
      "750:\tlearn: 4383.8472680\ttest: 4614.2332966\tbest: 4614.2332966 (750)\ttotal: 45.5s\tremaining: 1m 15s\n",
      "1000:\tlearn: 3725.7491003\ttest: 3981.5111195\tbest: 3981.5111195 (1000)\ttotal: 1m\tremaining: 1m\n",
      "1000:\tlearn: 3725.7491003\ttest: 3981.5111195\tbest: 3981.5111195 (1000)\ttotal: 1m\tremaining: 1m\n",
      "1250:\tlearn: 3249.5619834\ttest: 3529.7093317\tbest: 3529.7093317 (1250)\ttotal: 1m 15s\tremaining: 45.2s\n",
      "1250:\tlearn: 3249.5619834\ttest: 3529.7093317\tbest: 3529.7093317 (1250)\ttotal: 1m 15s\tremaining: 45.2s\n",
      "1500:\tlearn: 2889.1253222\ttest: 3183.6148921\tbest: 3183.6148921 (1500)\ttotal: 1m 30s\tremaining: 30.2s\n",
      "1500:\tlearn: 2889.1253222\ttest: 3183.6148921\tbest: 3183.6148921 (1500)\ttotal: 1m 30s\tremaining: 30.2s\n",
      "1750:\tlearn: 2589.0066804\ttest: 2892.2585161\tbest: 2892.2585161 (1750)\ttotal: 1m 46s\tremaining: 15.1s\n",
      "1750:\tlearn: 2589.0066804\ttest: 2892.2585161\tbest: 2892.2585161 (1750)\ttotal: 1m 46s\tremaining: 15.1s\n",
      "1999:\tlearn: 2333.8958008\ttest: 2641.9007213\tbest: 2641.9007213 (1999)\ttotal: 2m 1s\tremaining: 0us\n",
      "bestTest = 2641.900721\n",
      "bestIteration = 1999\n",
      "Fold 4 RMSE: 2641.8996\n",
      "1999:\tlearn: 2333.8958008\ttest: 2641.9007213\tbest: 2641.9007213 (1999)\ttotal: 2m 1s\tremaining: 0us\n",
      "bestTest = 2641.900721\n",
      "bestIteration = 1999\n",
      "Fold 4 RMSE: 2641.8996\n",
      "0:\tlearn: 34106.2962859\ttest: 34028.6216415\tbest: 34028.6216415 (0)\ttotal: 50.9ms\tremaining: 1m 41s\n",
      "0:\tlearn: 34106.2962859\ttest: 34028.6216415\tbest: 34028.6216415 (0)\ttotal: 50.9ms\tremaining: 1m 41s\n",
      "250:\tlearn: 7031.5580256\ttest: 7158.3642012\tbest: 7158.3642012 (250)\ttotal: 15s\tremaining: 1m 44s\n",
      "250:\tlearn: 7031.5580256\ttest: 7158.3642012\tbest: 7158.3642012 (250)\ttotal: 15s\tremaining: 1m 44s\n",
      "500:\tlearn: 5355.4795512\ttest: 5581.8915285\tbest: 5581.8915285 (500)\ttotal: 29.9s\tremaining: 1m 29s\n",
      "500:\tlearn: 5355.4795512\ttest: 5581.8915285\tbest: 5581.8915285 (500)\ttotal: 29.9s\tremaining: 1m 29s\n",
      "750:\tlearn: 4391.3923945\ttest: 4665.9817908\tbest: 4665.9817908 (750)\ttotal: 44.7s\tremaining: 1m 14s\n",
      "750:\tlearn: 4391.3923945\ttest: 4665.9817908\tbest: 4665.9817908 (750)\ttotal: 44.7s\tremaining: 1m 14s\n",
      "1000:\tlearn: 3742.7498590\ttest: 4042.3592050\tbest: 4042.3592050 (1000)\ttotal: 1m\tremaining: 59.9s\n",
      "1000:\tlearn: 3742.7498590\ttest: 4042.3592050\tbest: 4042.3592050 (1000)\ttotal: 1m\tremaining: 59.9s\n",
      "1250:\tlearn: 3254.4921224\ttest: 3566.0317553\tbest: 3566.0317553 (1250)\ttotal: 1m 14s\tremaining: 44.8s\n",
      "1250:\tlearn: 3254.4921224\ttest: 3566.0317553\tbest: 3566.0317553 (1250)\ttotal: 1m 14s\tremaining: 44.8s\n",
      "1500:\tlearn: 2863.1329121\ttest: 3185.0560969\tbest: 3185.0560969 (1500)\ttotal: 1m 29s\tremaining: 29.9s\n",
      "1500:\tlearn: 2863.1329121\ttest: 3185.0560969\tbest: 3185.0560969 (1500)\ttotal: 1m 29s\tremaining: 29.9s\n",
      "1750:\tlearn: 2547.1489483\ttest: 2875.9358514\tbest: 2875.9358514 (1750)\ttotal: 1m 44s\tremaining: 14.9s\n",
      "1750:\tlearn: 2547.1489483\ttest: 2875.9358514\tbest: 2875.9358514 (1750)\ttotal: 1m 44s\tremaining: 14.9s\n",
      "1999:\tlearn: 2294.3521283\ttest: 2624.8250580\tbest: 2624.8250580 (1999)\ttotal: 1m 59s\tremaining: 0us\n",
      "bestTest = 2624.825058\n",
      "bestIteration = 1999\n",
      "Fold 5 RMSE: 2624.8269\n",
      "1999:\tlearn: 2294.3521283\ttest: 2624.8250580\tbest: 2624.8250580 (1999)\ttotal: 1m 59s\tremaining: 0us\n",
      "bestTest = 2624.825058\n",
      "bestIteration = 1999\n",
      "Fold 5 RMSE: 2624.8269\n",
      "OOF RMSE: 2625.4101\n",
      "Bias correction factor: 1.0000\n",
      "Submission saved to: submissions\\submission_catboost_cv_2625.410.csv\n",
      "OOF RMSE: 2625.4101\n",
      "Bias correction factor: 1.0000\n",
      "Submission saved to: submissions\\submission_catboost_cv_2625.410.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stores_times_sizes</td>\n",
       "      <td>11.391924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lifecycle_per_store</td>\n",
       "      <td>8.974239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>price</td>\n",
       "      <td>6.928117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>demand_times_lifecycle</td>\n",
       "      <td>6.279555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>stores_plus_sizes</td>\n",
       "      <td>5.836264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>stores_minus_sizes</td>\n",
       "      <td>5.385672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>life_cycle_length</td>\n",
       "      <td>4.422606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>stores_per_week</td>\n",
       "      <td>4.092043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>lifecycle_weeks</td>\n",
       "      <td>3.487684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>num_stores</td>\n",
       "      <td>1.994340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>img_emb_cluster_freq</td>\n",
       "      <td>1.649659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>weekly_sales</td>\n",
       "      <td>1.273469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>aggregated_family</td>\n",
       "      <td>1.163633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>fabric</td>\n",
       "      <td>1.082485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>lifecycle_days</td>\n",
       "      <td>0.956278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>phase_out_freq</td>\n",
       "      <td>0.945160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>phase_in_dayofyear_sin</td>\n",
       "      <td>0.888264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>img_pca_3</td>\n",
       "      <td>0.867454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>img_emb_cluster</td>\n",
       "      <td>0.811501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>img_emb_max</td>\n",
       "      <td>0.810246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>img_pca_5</td>\n",
       "      <td>0.779887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>phase_out_week</td>\n",
       "      <td>0.764717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>color_name_freq</td>\n",
       "      <td>0.746236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>stores_per_size</td>\n",
       "      <td>0.732103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>num_sizes</td>\n",
       "      <td>0.721873</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   feature  importance\n",
       "0       stores_times_sizes   11.391924\n",
       "1      lifecycle_per_store    8.974239\n",
       "2                    price    6.928117\n",
       "3   demand_times_lifecycle    6.279555\n",
       "4        stores_plus_sizes    5.836264\n",
       "5       stores_minus_sizes    5.385672\n",
       "6        life_cycle_length    4.422606\n",
       "7          stores_per_week    4.092043\n",
       "8          lifecycle_weeks    3.487684\n",
       "9               num_stores    1.994340\n",
       "10    img_emb_cluster_freq    1.649659\n",
       "11            weekly_sales    1.273469\n",
       "12       aggregated_family    1.163633\n",
       "13                  fabric    1.082485\n",
       "14          lifecycle_days    0.956278\n",
       "15          phase_out_freq    0.945160\n",
       "16  phase_in_dayofyear_sin    0.888264\n",
       "17               img_pca_3    0.867454\n",
       "18         img_emb_cluster    0.811501\n",
       "19             img_emb_max    0.810246\n",
       "20               img_pca_5    0.779887\n",
       "21          phase_out_week    0.764717\n",
       "22         color_name_freq    0.746236\n",
       "23         stores_per_size    0.732103\n",
       "24               num_sizes    0.721873"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cross-validated CatBoost training and calibrated inference\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "oof_predictions = np.zeros(len(X))\n",
    "test_predictions = np.zeros(len(test_features))\n",
    "feature_importance = np.zeros(len(X.columns), dtype=float)\n",
    "\n",
    "catboost_base_params = dict(\n",
    "    iterations=2000,\n",
    "    depth=8,\n",
    "    learning_rate=0.025,\n",
    "    loss_function=\"RMSE\",\n",
    "    eval_metric=\"RMSE\",\n",
    "    l2_leaf_reg=6.0,\n",
    "    min_child_samples=25,\n",
    "    random_strength=0.4,\n",
    "    bagging_temperature=0.75,\n",
    "    od_type=\"Iter\",\n",
    "    od_wait=200,\n",
    "    verbose=250,\n",
    ")\n",
    "\n",
    "if CATBOOST_TASK_TYPE == \"GPU\":\n",
    "    catboost_base_params.update(\n",
    "        task_type=\"GPU\",\n",
    "        devices=GPU_DEVICES,\n",
    "        grow_policy=\"SymmetricTree\",\n",
    "        bootstrap_type=\"Bayesian\",\n",
    "        leaf_estimation_backtracking=\"AnyImprovement\",\n",
    "    )\n",
    "else:\n",
    "    catboost_base_params.update(\n",
    "        grow_policy=\"Lossguide\",\n",
    "        subsample=0.85,\n",
    "        colsample_bylevel=0.85,\n",
    "    )\n",
    "\n",
    "for fold, (train_idx, valid_idx) in enumerate(kf.split(X), start=1):\n",
    "    X_train_fold, X_valid_fold = X.iloc[train_idx], X.iloc[valid_idx]\n",
    "    y_train_fold, y_valid_fold = y.iloc[train_idx], y.iloc[valid_idx]\n",
    "\n",
    "    train_pool = Pool(X_train_fold, y_train_fold, cat_features=cat_features_for_pool)\n",
    "    valid_pool = Pool(X_valid_fold, y_valid_fold, cat_features=cat_features_for_pool)\n",
    "\n",
    "    fold_params = catboost_base_params.copy()\n",
    "    fold_params[\"random_seed\"] = SEED + fold\n",
    "    if CATBOOST_TASK_TYPE == \"GPU\":\n",
    "        fold_params.pop(\"subsample\", None)\n",
    "        fold_params.pop(\"colsample_bylevel\", None)\n",
    "    model = CatBoostRegressor(**fold_params)\n",
    "    model.fit(train_pool, eval_set=valid_pool, use_best_model=True)\n",
    "\n",
    "    fold_oof = model.predict(valid_pool)\n",
    "    oof_predictions[valid_idx] = fold_oof\n",
    "    fold_rmse = root_mean_squared_error(y_valid_fold, fold_oof)\n",
    "    print(f\"Fold {fold} RMSE: {fold_rmse:.4f}\")\n",
    "\n",
    "    test_predictions += model.predict(Pool(test_features, cat_features=cat_features_for_pool)) / kf.n_splits\n",
    "    feature_importance += model.get_feature_importance(type=\"FeatureImportance\")\n",
    "\n",
    "cv_rmse = root_mean_squared_error(y, oof_predictions)\n",
    "y_mean = y.mean()\n",
    "oof_mean = oof_predictions.mean()\n",
    "bias_correction = y_mean / oof_mean if np.isfinite(oof_mean) and abs(oof_mean) > 1e-6 else 1.0\n",
    "test_predictions = np.clip(test_predictions * bias_correction, 0, None)\n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    \"ID\": test_ids.values,\n",
    "    \"Production\": np.rint(test_predictions).astype(int)\n",
    "})\n",
    "submission_path = SUBMISSION_DIR / f\"submission_catboost_cv_{cv_rmse:.3f}.csv\"\n",
    "submission_df.to_csv(submission_path, index=False)\n",
    "\n",
    "feature_importance_df = (\n",
    "    pd.DataFrame({\"feature\": X.columns, \"importance\": feature_importance / kf.n_splits})\n",
    "    .sort_values(\"importance\", ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(f\"OOF RMSE: {cv_rmse:.4f}\")\n",
    "print(f\"Bias correction factor: {bias_correction:.4f}\")\n",
    "print(f\"Submission saved to: {submission_path}\")\n",
    "feature_importance_df.head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065d8e35",
   "metadata": {},
   "source": [
    "## GPU runtime notes\n",
    "- CatBoost now auto-detects CUDA availability via `get_gpu_device_count()` and switches to `task_type=\"GPU\"` on device string `\"0\"` (edit `GPU_DEVICES` if you want multi-GPU like `\"0-1\"`).\n",
    "- Training matrices downcast numeric columns to `float32` before constructing `Pool` objects, which reduces host memory pressure and shortens PCIe transfer time on the RTX 3060.\n",
    "- When no compatible GPU is present, the notebook falls back to CPU training automatically while keeping identical hyperparameters aside from `grow_policy`.\n",
    "- Ensure NVIDIA drivers are up to date; CatBoost wheels already ship with the matching CUDA runtime on Windows, so no manual toolkit install is required in most setups."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
